![Latent Board State](/figures/latent_board_state_reward_tsne.png)

# Transcendence Chess Research

<div align="center">

[![Build status](https://github.com/ezhang7423/language-control-diffusion/workflows/build/badge.svg?branch=master&event=push)](https://github.com/ezhang7423/chess_research/pulls)
[![Dependencies Status](https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg)](https://github.com/ezhang7423/language-control-diffusion/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Aapp%2Fdependabot)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Utility: isort](https://img.shields.io/badge/imports-isort-orange.svg)](https://pycqa.github.io/isort/)
[![Pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/ezhang7423/chess_research/blob/main/.pre-commit-config.yaml)
[![Semantic Versions](https://img.shields.io/badge/%20%20%F0%9F%93%A6%F0%9F%9A%80-semantic--versions-e10079.svg)](https://github.com/ezhang7423/chess_research/releases)
[![License](https://img.shields.io/github/license/ezhang7423/language-control-diffusion)](https://github.com/ezhang7423/chess_research/blob/main/LICENSE)

Generative Models Can Outperform The Experts That Train Them


</div>

Generative models are trained with the simple objective of imitating the conditional probability distribution induced by the data they are trained on. Therefore, when trained on data generated by human experts, we may not expect the artificial model to outperform the experts on their original objectives. Yet it is often observed in practice that such models possess surprising capabilities, suggesting that they might surpass human experts in certain aspects.

In this work, we study the phenomenon of transcendence: when a generative model achieves capabilities that surpass the abilities of the human experts generating its data. We demonstrate transcendence by training an autoregressive transformer to play chess from game transcripts, and show that the trained model can sometimes achieve better Glicko-2 scores compared to the players in the dataset.

We theoretically prove that transcendence is enabled by low-temperature sampling, and rigorously assess this experimentally. Finally, we discuss other forms of transcendence, laying the groundwork for future investigation of this phenomenon in a broader setting.

### How Transcendence Works through Low-Temperature Sampling
Visualizing the denoising effects of low temperature on the action distribution: an example of ChessFormer shifting probability mass towards the high reward move of trapping the queen with the rook as the temperature ğœ decreases. Opacity of the red arrows represent the probability mass given to different moves. The color of the square represent the reward that would be given for taking the action that moves the given piece to that state. Purple here is high reward, while blue is low.

![Low Temperature](/figures/advantage-analysis.png)

![nanoGPT](/figures/rating_temp.png)

Here are the ratings of our autoregressive decoder-only transformer, ChessFormer, over several different temperatures. Each model is trained only on games with players up to a certain rating (1000, 1300, 1500), respectively. Interestingly, ChessFormer 1500 is unable to transcend at test time, a result that we further analyze in the paper. The results do give us quite a conclusive argument of the denoising effects of lower temperatures

### t-SNE Embeddings of ChessFormer
For the image at the very top, try zooming in by right clicking on the image, and click "Open Image in New Tab". Inspired by Deep Q-Networks (DQN), we generate a t-SNE embedding of ChessFormer's last hidden layer latent representations of game transcripts during training time. The colors represent the probability of winning, with $+1$ corresponding to a state where White has won and -1 to Black. We also visualize several board states associated different clusters within the t-SNE embedding, and their associated expected reward when following the expert Stockfish distribution.

## ğŸš€ Features

This repository contains months of research aimed towards demonstrating the phenomenon we have coined "Transcendence". The training code, the PyTorch modeling framework used, and evaluation against Stockfish all sit inside of the inner chess_research directory. We emplore the community to try the training and evaluation out for themselves. See if these results are reproducable and if there are other interesting phenomenon to be observed.

- The scripts directory provides a template for running the trainings or evaluations desired. 

- There are a few different config.json files that are ready for use immediately. 

- The nanogpt_module.py processes the game strings that are passed into the GPT model we used. 

- The [`dataset-viz`](https://github.com/ezhang7423/chess_research/tree/dataset-viz) branch provides code for analysing the dataset used and for generating the figures discussed below. 

- The [`advantage-analysis`](https://github.com/ezhang7423/chess_research/tree/advantaage-analysis) branch dives into how the stockfish engine is used to calculate the reward of each move made in a game. This is actually how the analysis is generated for a game on Lichess.org. Look into this [post](https://www.landonlehman.com/post/2021-01-25-how-to-reproduce-a-lichess-advantage-chart-in-python/) for a better intuition of what was being evaluated here.

- There is also the integration of the [`Glicko2`](https://github.com/fsmosca/glicko2calculator) repository, the method of calcuating the elo ratings of players over a series of games. it is simple to use and adjust to your preference.

- There are other branhces that contain work for other experimental settings. We are looking into these settings as future work, but feel free to play with the available code now. The What's Next section of this READ.ME.md gives some more indepth explanations of what was trying to be achieved in these settings.

## ğŸ§ª Experiment Logs

The experiments for the Max_Elo 1000 are logged [here](https://wandb.ai/project-eval/50M-Training/reports/Transcendence-Chess-Research---Vmlldzo4MzAxODA2?accessToken=9r9uih3djihscx3w67h47dfeh9rynd69toc001mr0a9qzqa2cxvie9izlu8yomp1).

## Installation
We require [`Conda`](https://docs.conda.io/en/latest/miniconda.html) for this repository. To install, simply run

```bash
$ make install
$ source .vevn/bin/activate
```

This will set up a new conda environment with [`Poetry`](https://python-poetry.org/) as the dependencies manager.

To download models and dataset, we recommend installing [`git lfs`](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage) to interface with our [`huggingface`](https://huggingface.co/docs/hub/en/repositories-getting-started) repos that house the data and models. You can find those repos here:

- [50M Parameter Models for Ratings 1000 - 1500](https://huggingface.co/datasets/ezipe/lichess-models/tree/main)
- [Massive Dataset of Chess Games from Lichess.org split by Different Max Ratings](https://huggingface.co/datasets/ezipe/lichess_elo_binned_debug/tree/main)


## Usage

To train your own models from scratch, set up a configuration json file that match your specifications. Here is an example of our [50M parameter model config file](https://github.com/ezhang7423/chess_research/blob/main/config/50M_1000.json).

Next, run the following

```
python chess_research --config $PWD/config/your_config.json
```

To use arguments, you can run like the following (this is an example of running a training with max rating 1500 and evaluating with different temperatures):

```
python chess_research --config $PWD/config/your_config.json --temperature_sampling true --high_elo 1500
```

To resume from a prior run

```
python chess_research --resume_from $PWD/runs/50M-High-Elo-1000-No-Elo-Conditioning/2024-04-30---03-05-07_50M-High-Elo-1000-No-Elo-Conditioning --resume_iter_num 100000
```

Here is an explanation of some useful arguments to look at that we used to run our experiments and their purpose:

- temperature_sampling: if true, will run evaluations with temperatures [0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 1.0], used to denoise the models
- eval_only: if true, will not run the training, only an evaluation of the model passed in against Stockfish (levels 1, 3, 5)
- high_elo: sets the maximum elo ratings of the games that the model will see during train time
- wandb_log: if true, will send the data of the training to a wandb project that you set up and authenticate, to view your results.
- resume_from: sets the path of which directory to resume the training from. The directory should include the weight files and the config.json necessary to continue training


Distributed Data Parallel (DDP):
DDP is incorparated into the repo to speed up training by running on multiple GPUs in parallel (model params are duplicated, batch size = original_batch_size * num_gpus). 
To run with ddp,
```
torchrun --standalone --nproc_per_node=4  chess_research/__init__.py -c $PWD/config/350_1100_elo_gen.json 
```

Tips:
- Use tmux to run in the background.
- There is a script for running multiple trainings at different high elos that take in different configuration files in the scripts directory found [here](https://github.com/ezhang7423/chess_research/blob/cleanup/scripts/train_big.py).
- Just run the scripts with ipython or python in the terminal; they will be running in the background .

## ğŸ—ï¸ Development

### Directory Structure

```
CHESS_RESEARCH
â”œâ”€â”€ .devcontainer
â”œâ”€â”€ .empty
â”œâ”€â”€ .github
â”œâ”€â”€ .venv
â”œâ”€â”€ .vscode
â”œâ”€â”€ adam-chess-data
â”œâ”€â”€ chess_research
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â””â”€â”€ zstd_process.py
â”‚   â”œâ”€â”€ eval
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ wandb
â”‚   â”‚   â”œâ”€â”€ data_structures.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”‚   â”œâ”€â”€ glicko2.py
â”‚   â”‚   â”œâ”€â”€ player.py
â”‚   â”‚   â”œâ”€â”€ utils.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ globals.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train.py
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ 50M_1000.json
â”‚   â”œâ”€â”€ 302M_1000.json
â”‚   â”œâ”€â”€ 707M_1000.json
â”œâ”€â”€ length_gen_evals
â”œâ”€â”€ lichess_hf_dataset
â”œâ”€â”€ scripts
|   â”œâ”€â”€ train_big.py
â”œâ”€â”€ tactics
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ runs
```


### Poetry

Want to know more about Poetry? Check [its documentation](https://python-poetry.org/docs/).

<details>
<summary>Details about Poetry</summary>
<p>

Poetry's [commands](https://python-poetry.org/docs/cli/#commands) are very intuitive and easy to learn, like:

- `poetry add numpy@latest`
- `poetry run pytest`
- `poetry publish --build`

etc

</p>
</details>

## ğŸ¯ What's next
The temperture sampling for denoising is only the beginning. The main branch also contains code for a elo conditioning experiment. Moreover, as mentioned above, there were many other experimental settings this project can explore. In the [length_generalization](https://github.com/ezhang7423/chess_research/tree/length_generalization) and [win_condition](https://github.com/ezhang7423/chess_research/tree/win_condition) branches of this repository, you can explore some of our preliminary work. Here is a brief summary and explanatin of intention for each setting.

- [`length_generalization`](https://github.com/ezhang7423/chess_research/tree/length_generalization): We want to see if the model can perform out of distribution. In this setting, we train the model with games in a datset of a maximum length instead of maximum elo rating. Then, during test time, we prompt with already played out games and have it start with a certian number of starting moves that are already played in that game. The ideal outcome we wanted to see is if the model can do well even when the games go beyond the maximum length that it was trained on.

- [`win_condition`](https://github.com/ezhang7423/chess_research/tree/win_condition): The idea behind win conditioning is that the result of each game from the dataset that the model sees during train time already tells the model who is going to win with a W or a L prompt that comes before the game string PGN and marks either white will win and black will loss or white will loss and black will win. Then, during test time, we prompt the model by prepending a W to the start of the game state PGN passed in. We want to see if this kind of conditining can improve the model's performance in terms of rating when put through the evaluation against Stockfish. The ideal outcome here is that the model will perform better given this conditiing of seeing a winning marker 'W' at the start of all the game string PGNs.

- elo_conditioning: This is not a separate branch, but integrated into the main branch. It is a configurable setting in the config.json. This setting is very similar to the win_conditioning example: the games that the model sees during training have a prompt that tells the model the elo of the white player and the elo fo the black player. Note, the games in the training dataset still have a macimum rating. Then, during test time, the game string PGN are also marked with an elo that is greater than the maximum rating that the model was trained with. We want to see if the model can transcend from the prompting of higher elos.

## ğŸ‘ Credits

### Massive Thanks to Adam Karvonen
Adam Karvonen studied the application of language models to playing chess, building on previous ML research, including gpt-3.5-turbo-instruct's ability to play chess at an 1800 Elo level, and Kenneth Li's work on Emergent World Representations. You can take a look at his work [here](https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html)
Karvonen hypothesized that the poor performance of Othello-GPT trained on human-played games was due to insufficient data. To test this, Karvonen trained a 50 million parameter GPT on 5 million chess games, using both synthetic superhuman bot games and 16 million real games from the Lichess database.
The resulting model could predict the next move in a game with high accuracy and play chess at approximately 1300 Elo after only a day of training on 4 RTX 3090 GPUs. This model, which was never given explicit board states or rules, learned a variety of chess concepts and could estimate latent variables like player Elo ratings, emphasizing that a large language model can indeed learn chess from a large dataset of games.
The probes were very accurate (99.2%) in classifying each square as one of 13 classes (blank, or containing a white or black pawn, rook, bishop, knight, king, or queen), suggesting that the model indeed formed an internal representation of the board.
To visualize how the model was tracking the board state, Karvonen created visual heat maps to show the probe's confidence in the presence of specific pieces at given squares. These heat maps confirmed the model's strong internal representation of the board, clearly showing its confidence level in the placement of pieces, indicated by gradients in the probe's output.
Moreover, Karvonen investigated whether the model could infer the skill level of the players, a latent variable, by designing probes to predict player Elo based on model activations during mid-game. The linear probe accuracy was competing with that of a randomly initialized model, but when tasked to classify players as below 1550 or above 2050 Elo, the probe performed significantly better, indicating that the model could indeed estimate player skill level to some degree.
He suggested that his findings indicated that large neural networks, when trained to predict the next tokens in a dataset, could pick up underlying patterns and form internal representations about the data they're predicting, as seen in past

### Thank you Lichess.org
Lichess gave us many resources for calculating ratings, advantages, and running games with our model and other pre-established models.
The [lichess-bot](https://github.com/lichess-bot-devs/lichess-bot) repository was a huge help in getting up to date information on Stockfish and allowing us to build our own bot with the models that we trained. Our [`ChessFormer-Bot`](https://lichess.org/@/ChessFormer-1000) trained with a max rating of 1000 is up and running on Lichess.org and has been performing significantly better against other bots (maia1, maia5, maia9). Additionally, you can challange our bot to a real-time game yourself at this [link](https://lichess.org/?user=ChessFormer-1000#friend), if you want to see it in action.

This project would have been much tougher to complete without these resources and past works, so thank you!